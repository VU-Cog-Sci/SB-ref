{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show PA maps in browser\n",
    "# usefull to check if meridians well drawn\n",
    "\n",
    "# make figure of inflated PA maps\n",
    "# and soma maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_sharedLM = '/Users/verissimo/disks/shared_aeneas/2018/visual/SB-prep/SB-ref/derivatives/'\n",
    "folder_homeLM = '/Users/verissimo/disks/verissimo_aeneas/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200408-13:53:35,886 nipype.utils INFO:\n",
      "\t No new version available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/nipype/workflows/__init__.py:28: UserWarning: Nipype 1 workflows have been moved to the niflow-nipype1-workflows package. pip install niflow-nipype1-workflows to continue using them.\n",
      "  warnings.warn(\" \".join(_msg))\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/numba/decorators.py:29: NumbaDeprecationWarning: autojit is deprecated, use jit instead, which provides the same functionality. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-numba-autojit\n",
      "  warnings.warn(NumbaDeprecationWarning(msg))\n",
      "Using TensorFlow backend.\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "import cortex\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "from utils import *\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.cm as matcm\n",
    "import matplotlib.pyplot as plt\n",
    "from distutils.util import strtobool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sj = '01'\n",
    "with open('analysis_params.json','r') as json_file:\t\n",
    "            analysis_params = json.load(json_file)\t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "with_smooth = 'True'#'False'#analysis_params['with_smooth']\n",
    "\n",
    "# path to save figure\n",
    "figure_out = os.path.join(folder_sharedLM,'figures','multimodal','sub-{sj}'.format(sj=sj))\n",
    "# dir to get soma contrasts\n",
    "soma_dir = os.path.join(folder_sharedLM,'soma_fit','new_fits','sub-{sj}'.format(sj=sj)) \n",
    "\n",
    "if with_smooth=='True':\n",
    "    figure_out = os.path.join(figure_out,'smooth%d'%analysis_params['smooth_fwhm'])\n",
    "    soma_dir = os.path.join(soma_dir,'smooth%d'%analysis_params['smooth_fwhm'])\n",
    "\n",
    "if not os.path.exists(figure_out): # check if path to save figures exists\n",
    "    os.makedirs(figure_out) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRF PARAMS #######\n",
    "# fit model to use (gauss or css)\n",
    "fit_model = 'css' #analysis_params[\"fit_model\"]\n",
    "\n",
    "# if using estimates from iterative fit\n",
    "iterative_fit = True #True\n",
    "\n",
    "# total number of chunks that were fitted (per hemi)\n",
    "total_chunks = analysis_params['total_chunks']\n",
    "    \n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading visual xx from /Users/verissimo/disks/shared_aeneas/2018/visual/SB-prep/SB-ref/derivatives/figures/prf/final_fig/css/iterative/sub-01/smooth2/xx_pRF_fitmodel-css_itertivefit-True.npy\n",
      "loading visual yy from /Users/verissimo/disks/shared_aeneas/2018/visual/SB-prep/SB-ref/derivatives/figures/prf/final_fig/css/iterative/sub-01/smooth2/yy_pRF_fitmodel-css_itertivefit-True.npy\n"
     ]
    }
   ],
   "source": [
    "### POLAR ANGLES MAPS #####\n",
    "# now load estimates to make polar angle\n",
    "# should have run make_PA4drawing script first\n",
    "\n",
    "prf_path = os.path.join(folder_sharedLM,'figures','prf','final_fig',fit_model)\n",
    "\n",
    "if iterative_fit==True:\n",
    "    prf_path = os.path.join(prf_path,'iterative','sub-{sj}'.format(sj=sj))\n",
    "else:\n",
    "    prf_path = os.path.join(prf_path,'grid','sub-{sj}'.format(sj=sj))\n",
    "    \n",
    "if with_smooth=='True':\n",
    "    prf_path = os.path.join(prf_path,'smooth%d'%analysis_params['smooth_fwhm'])\n",
    "    \n",
    "# filenames\n",
    "xx_filename = os.path.join(prf_path,'xx_pRF_fitmodel-%s_itertivefit-%s.npy'%(fit_model,str(iterative_fit)))\n",
    "yy_filename = os.path.join(prf_path,'yy_pRF_fitmodel-%s_itertivefit-%s.npy'%(fit_model,str(iterative_fit)))\n",
    "\n",
    "# load\n",
    "print('loading visual xx from %s'%xx_filename)\n",
    "masked_xx = np.load(xx_filename,allow_pickle=True)\n",
    "\n",
    "print('loading visual yy from %s'%yy_filename)\n",
    "masked_yy = np.load(yy_filename,allow_pickle=True)\n",
    "\n",
    "rsq_threshold = 0.14\n",
    "\n",
    "complex_location = masked_xx + masked_yy * 1j\n",
    "masked_polar_angle = np.angle(complex_location)\n",
    "masked_eccentricity = np.abs(complex_location)    \n",
    "\n",
    "images = {}\n",
    "\n",
    "# normalize polar angles to have values in circle between 0 and 1\n",
    "masked_polar_ang_norm = (masked_polar_angle + np.pi) / (np.pi * 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading visual rsq from /Users/verissimo/disks/shared_aeneas/2018/visual/SB-prep/SB-ref/derivatives/figures/multimodal/sub-01/smooth2/rsq_pRF_fitmodel-css_itertivefit-True.npy\n"
     ]
    }
   ],
   "source": [
    "# filename for visual rsq\n",
    "rsq_visual_filename = os.path.join(figure_out,'rsq_pRF_fitmodel-%s_itertivefit-%s.npy'%(fit_model,str(iterative_fit)))\n",
    "print('loading visual rsq from %s'%rsq_visual_filename)\n",
    "rsq_visual = np.load(rsq_visual_filename,allow_pickle=True)\n",
    "# do this to replace nans with 0s, so flatmaps look nicer\n",
    "#rsq_visual = np.array([x if np.isnan(x)==False else 0 for _,x in enumerate(rsq_visual)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### make PA flatmaps with non uniform color wheel ######\n",
    "# shift radians in order to overrepresent red color\n",
    "# useful to make NON-REPRESENTED retinotopic hemifield per hemisphere red\n",
    "# then easier to define borders\n",
    "\n",
    "# create HSV array, with PA values (-pi to pi) that were obtained from estimates\n",
    "# saturation wieghted by a shifted distribution of RSQ (better visualization)\n",
    "# value bolean (if I don't give it an rsq threshold then it's always 1)\n",
    "\n",
    "hsv_angle = []\n",
    "hsv_angle = np.ones((len(rsq_visual), 3))\n",
    "hsv_angle[:, 0] = masked_polar_angle.copy()\n",
    "#hsv_angle[:, 1] = np.clip(rsq_visual / np.nanmax(rsq_visual) * 3, 0, 1)\n",
    "hsv_angle[:, 2] = rsq_visual > rsq_threshold #np.clip(rsq_visual / np.nanmax(rsq_visual) * 3, 0, 1)#rsq_visual > 0.12 #rsq_threshold\n",
    "\n",
    "# get mid vertex index (diving hemispheres)\n",
    "left_index = cortex.db.get_surfinfo('fsaverage').left.shape[0] \n",
    "\n",
    "### take angles from LH (thus RVF)##\n",
    "angle_ = hsv_angle[:left_index, 0].copy()\n",
    "angle_thresh = 3*np.pi/4 #value upon which to make it red for this hemifield (above it or below -angle will be red)\n",
    "\n",
    "#normalized angles, between 0 and 1\n",
    "hsv_angle[:left_index, 0] = np.clip((angle_ + angle_thresh)/(2*angle_thresh), 0, 1)\n",
    "\n",
    "### take angles from RH (thus LVF) ##\n",
    "angle_ = -hsv_angle[left_index:, 0].copy() # ATENÇÃO -> minus sign to flip angles vertically (then order of colors same for both hemispheres)\n",
    "\n",
    "# sum 2pi to originally positive angles (now our trig circle goes from pi to 2pi to pi again, all positive)\n",
    "angle_[hsv_angle[left_index:, 0] > 0] += 2 * np.pi\n",
    "\n",
    "#normalized angles, between 0 and 1\n",
    "angle_ = np.clip((angle_ + (angle_thresh-np.pi))/(2*angle_thresh), 0, 1) # ATENÇÃO -> we subtract -pi to angle thresh because now we want to rotate the whole thing -180 degrees\n",
    "\n",
    "hsv_angle[left_index:, 0] = angle_.copy()\n",
    "rgb_angle = []\n",
    "rgb_angle = colors.hsv_to_rgb(hsv_angle)\n",
    "\n",
    "# make alpha same as saturation, reduces clutter\n",
    "alpha_angle = hsv_angle[:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images['angle_half_hemi'] = cortex.VertexRGB(rgb_angle[:, 0], rgb_angle[:, 1], rgb_angle[:, 2],\n",
    "                                   alpha=alpha_angle,\n",
    "                                   subject='fsaverage_meridians')\n",
    "#cortex.quickshow(images['angle_half_hemi'],with_curvature=True,with_sulci=True,with_colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-12-2cc0479eeab7>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-2cc0479eeab7>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "# Name of a sub-layer of the 'cutouts' layer in overlays.svg file\n",
    "cutout_name = 'zoom_roi_right'\n",
    "_ = cortex.quickflat.make_figure(images['angle_half_hemi'],\n",
    "                                 with_curvature=True,\n",
    "                                 with_sulci=True,\n",
    "                                 with_roi=False,\n",
    "                                 with_colorbar=False,\n",
    "                                 cutout=cutout_name,height=2048)\n",
    "filename = os.path.join(figure_out,cutout_name+'_space-fsaverage_type-PA_bins-%d.svg'%n_bins)\n",
    "print('saving %s' %filename)\n",
    "_ = cortex.quickflat.make_png(filename, images['angle_half_hemi'], recache=True,with_colorbar=False,\n",
    "                              cutout=cutout_name,with_curvature=True,with_sulci=True,with_roi=False,height=2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for all fingers and appending in list\n",
      "Computing center of mass for left hand fingers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verissimo/disks/verissimo_aeneas/SB-ref/analysis/utils.py:381: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  center_of_mass.append(sum(np.multiply(elem_num,elemz_thresh))/sum(elemz_thresh))\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing center of mass for right hand fingers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verissimo/anaconda3/envs/i36/lib/python3.6/site-packages/matplotlib/colors.py:527: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for each face part and appending in list\n",
      "Computing center of mass for face elements ['eyebrows', 'eyes', 'mouth', 'tongue']\n"
     ]
    }
   ],
   "source": [
    "## NOW DO SOMA PLOTS ###\n",
    "rsq_threshold = 0 \n",
    "z_threshold = analysis_params['z_threshold'] #2.7 #\n",
    "\n",
    "# load contrasts for different regions\n",
    "face_contrast = np.load(os.path.join(soma_dir,'z_face_contrast_rsq-%.2f.npy' %(rsq_threshold)))\n",
    "hand_contrast = np.load(os.path.join(soma_dir,'z_upper_limb_contrast_rsq-%.2f.npy' %(rsq_threshold)))\n",
    "\n",
    "# plot different body areas\n",
    "# but first threshold them (only show positive z-scores)\n",
    "data_threshed_face = zthresh(face_contrast,threshold=z_threshold,side='above')\n",
    "data_threshed_hand = zthresh(hand_contrast,threshold=z_threshold,side='above')\n",
    "\n",
    "# mask to only show relevant voxels\n",
    "rl_mask = np.array([True if np.isnan(val) else False for _,val in enumerate(data_threshed_hand)])\n",
    "\n",
    "\n",
    "# all fingers in hand combined\n",
    "LHfing_zscore = [] # load and append each finger z score in left hand list\n",
    "RHfing_zscore = [] # load and append each finger z score in right hand list\n",
    "\n",
    "\n",
    "print('Loading data for all fingers and appending in list')\n",
    "\n",
    "for i in range(len(analysis_params['all_contrasts']['upper_limb'])//2):\n",
    "    \n",
    "    LHfing_zscore.append(np.load(os.path.join(soma_dir,'z_%s-all_lhand_contrast_thresh-%0.2f_rsq-%.2f.npy' \n",
    "                                 %(analysis_params['all_contrasts']['upper_limb'][i],z_threshold,rsq_threshold))))\n",
    "    RHfing_zscore.append(np.load(os.path.join(soma_dir,'z_%s-all_rhand_contrast_thresh-%0.2f_rsq-%.2f.npy' \n",
    "                                              %(analysis_params['all_contrasts']['upper_limb'][i+5],z_threshold,rsq_threshold))))\n",
    "   \n",
    "\n",
    "\n",
    "LHfing_zscore = np.array(LHfing_zscore)\n",
    "RHfing_zscore = np.array(RHfing_zscore)\n",
    "\n",
    "# compute center of mass and appropriate z-scores for each hand\n",
    "print('Computing center of mass for left hand fingers')\n",
    "LH_COM , LH_avgzval = zsc_2_COM(LHfing_zscore)\n",
    "print('Computing center of mass for right hand fingers')\n",
    "RH_COM , RH_avgzval = zsc_2_COM(RHfing_zscore)\n",
    "\n",
    "\n",
    "# all fingers left hand combined ONLY in left hand region \n",
    "# (as defined by LvsR hand contrast values)\n",
    "\n",
    "LH_COM_4plot = LH_COM.copy()\n",
    "LH_COM_4plot[rl_mask] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "images = {}\n",
    "\n",
    "#images['v_Lfingers'] = cortex.Vertex(LH_COM_4plot, 'fsaverage_meridians',\n",
    "#                           vmin=0, vmax=4,\n",
    "#                           cmap='rainbow_r').raw#costum colormap added to database\n",
    "images['v_Lfingers'] = make_raw_vertex_image(LH_COM_4plot,'rainbow_r',0,4,subject='fsaverage_meridians')\n",
    "\n",
    "#cortex.quickshow(images['v_Lfingers'],with_curvature=True,with_sulci=True,with_colorbar=True)\n",
    "\n",
    "\n",
    "# all fingers right hand combined ONLY in right hand region \n",
    "# (as defined by LvsR hand contrast values)\n",
    "\n",
    "RH_COM_4plot = RH_COM.copy()\n",
    "RH_COM_4plot[rl_mask] = np.nan\n",
    "\n",
    "#images['v_Rfingers'] = cortex.Vertex(RH_COM_4plot, 'fsaverage_meridians',\n",
    "#                           vmin=0, vmax=4,\n",
    "#                           cmap='rainbow_r').raw#costum colormap added to database\n",
    "\n",
    "images['v_Rfingers'] = make_raw_vertex_image(RH_COM_4plot,'rainbow_r',0,4,subject='fsaverage_meridians')\n",
    "\n",
    "#cortex.quickshow(images['v_Rfingers'],with_curvature=True,with_sulci=True,with_colorbar=True)\n",
    "\n",
    "\n",
    "# all individual face regions combined\n",
    "\n",
    "allface_zscore = [] # load and append each face part z score in list\n",
    "\n",
    "print('Loading data for each face part and appending in list')\n",
    "\n",
    "for _,name in enumerate(analysis_params['all_contrasts']['face']):\n",
    "    \n",
    "    facedata = np.load(os.path.join(soma_dir,'z_%s-other_face_areas_contrast_thresh-%0.2f_rsq-%.2f.npy' %(name,z_threshold,rsq_threshold)))   \n",
    "    allface_zscore.append(facedata)  \n",
    "\n",
    "allface_zscore = np.array(allface_zscore)\n",
    "\n",
    "# combine them all in same array\n",
    "\n",
    "print('Computing center of mass for face elements %s' %(analysis_params['all_contrasts']['face']))\n",
    "allface_COM , allface_avgzval = zsc_2_COM(allface_zscore)\n",
    "\n",
    "\n",
    "# threshold left vs right, to only show relevant vertex \n",
    "# (i.e., where zscore is \"significant\", use it to mask face for plotting)\n",
    "face_mask = np.array([True if np.isnan(val) else False for _,val in enumerate(data_threshed_face)])\n",
    "\n",
    "allface_COM_4plot = allface_COM.copy()\n",
    "allface_COM_4plot[face_mask] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# 'eyebrows', 'eyes', 'mouth','tongue', , combined\n",
    "#images['v_facecombined'] = cortex.Vertex(allface_COM_4plot, 'fsaverage_meridians',\n",
    "#                           vmin=0, vmax=3,\n",
    "#                           cmap='J4')#costum colormap added to database\n",
    "\n",
    "images['v_facecombined'] = make_raw_vertex_image(allface_COM_4plot,'J4',0,3,subject='fsaverage_meridians')\n",
    "\n",
    "#cortex.quickshow(images['v_facecombined'],with_curvature=True,with_sulci=True,with_colorbar=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating new ctm file...\n",
      "wm\n",
      "wm\n",
      "inflated\n",
      "inflated\n",
      "Started server on port 3775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<JS: window.viewer>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = cortex.Dataset(#polar = images['angle_half_hemi'],\n",
    "                   face = images['v_facecombined'],\n",
    "                   Lhand = images['v_Lfingers'],\n",
    "                   Rhand = images['v_Rfingers'])\n",
    "\n",
    "cortex.webshow(data=ds,recache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 64252\n",
      "{'camera.azimuth': 180, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 10, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "Stopping server\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d_views_screenshots/LHand_meridians/sub-01_lateral_pivot_inflated.png',\n",
       " '3d_views_screenshots/LHand_meridians/sub-01_left_fiducial.png',\n",
       " '3d_views_screenshots/LHand_meridians/sub-01_right_fiducial.png',\n",
       " '3d_views_screenshots/LHand_meridians/sub-01_left_inflated.png',\n",
       " '3d_views_screenshots/LHand_meridians/sub-01_right_inflated.png',\n",
       " '3d_views_screenshots/LHand_meridians/sub-01_flatmap_flatmap.png',\n",
       " '3d_views_screenshots/LHand_meridians/sub-01_flatmap_flatmap.png']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_path = '3d_views_screenshots/LHand_meridians'\n",
    "if not os.path.exists(fig_path):  # check if path exists\n",
    "        os.makedirs(fig_path)\n",
    "\n",
    "cortex.export.save_3d_views(images['v_Lfingers'], \n",
    "             base_name=os.path.join(fig_path,'sub-%s'%(sj)), \n",
    "             list_angles=['lateral_pivot','left','right','left','right',\n",
    "                         'left','right'],\n",
    "                  list_surfaces=['inflated','fiducial','fiducial','inflated','inflated',\n",
    "                                'flatmap','flatmap'],\n",
    "                  viewer_params=dict(labels_visible=[],\n",
    "                                     overlays_visible=['rois','sulci']),\n",
    "                  size=(1024 * 4, 768 * 4), trim=True, sleep=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 61416\n",
      "{'camera.azimuth': 180, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 10, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "Stopping server\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d_views_screenshots/RHand_meridians/sub-01_lateral_pivot_inflated.png',\n",
       " '3d_views_screenshots/RHand_meridians/sub-01_left_fiducial.png',\n",
       " '3d_views_screenshots/RHand_meridians/sub-01_right_fiducial.png',\n",
       " '3d_views_screenshots/RHand_meridians/sub-01_left_inflated.png',\n",
       " '3d_views_screenshots/RHand_meridians/sub-01_right_inflated.png',\n",
       " '3d_views_screenshots/RHand_meridians/sub-01_flatmap_flatmap.png',\n",
       " '3d_views_screenshots/RHand_meridians/sub-01_flatmap_flatmap.png']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_path = '3d_views_screenshots/RHand_meridians'\n",
    "if not os.path.exists(fig_path):  # check if path exists\n",
    "        os.makedirs(fig_path)\n",
    "\n",
    "cortex.export.save_3d_views(images['v_Rfingers'], \n",
    "             base_name=os.path.join(fig_path,'sub-%s'%(sj)), \n",
    "             list_angles=['lateral_pivot','left','right','left','right',\n",
    "                         'left','right'],\n",
    "                  list_surfaces=['inflated','fiducial','fiducial','inflated','inflated',\n",
    "                                'flatmap','flatmap'],\n",
    "                  viewer_params=dict(labels_visible=[],\n",
    "                                     overlays_visible=['rois','sulci']),\n",
    "                  size=(1024 * 4, 768 * 4), trim=True, sleep=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 58994\n",
      "{'camera.azimuth': 180, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 10, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "Stopping server\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d_views_screenshots/Face_meridians/sub-01_lateral_pivot_inflated.png',\n",
       " '3d_views_screenshots/Face_meridians/sub-01_left_fiducial.png',\n",
       " '3d_views_screenshots/Face_meridians/sub-01_right_fiducial.png',\n",
       " '3d_views_screenshots/Face_meridians/sub-01_left_inflated.png',\n",
       " '3d_views_screenshots/Face_meridians/sub-01_right_inflated.png',\n",
       " '3d_views_screenshots/Face_meridians/sub-01_flatmap_flatmap.png',\n",
       " '3d_views_screenshots/Face_meridians/sub-01_flatmap_flatmap.png']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_path = '3d_views_screenshots/Face_meridians'\n",
    "if not os.path.exists(fig_path):  # check if path exists\n",
    "        os.makedirs(fig_path)\n",
    "\n",
    "cortex.export.save_3d_views(images['v_facecombined'], \n",
    "             base_name=os.path.join(fig_path,'sub-%s'%(sj)), \n",
    "             list_angles=['lateral_pivot','left','right','left','right',\n",
    "                         'left','right'],\n",
    "                  list_surfaces=['inflated','fiducial','fiducial','inflated','inflated',\n",
    "                                'flatmap','flatmap'],\n",
    "                  viewer_params=dict(labels_visible=[],\n",
    "                                     overlays_visible=['rois','sulci']),\n",
    "                  size=(1024 * 4, 768 * 4), trim=True, sleep=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 50279\n",
      "{'camera.azimuth': 180, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 10, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 90, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 270, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 0, 'camera.altitude': 180, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 179.9 -> 180\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 90, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 0.5, 'surface.{subject}.pivot': 0, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "{'camera.azimuth': 180, 'camera.altitude': 0, 'camera.target': [0, 0, 0], 'surface.{subject}.unfold': 1, 'surface.{subject}.pivot': 180, 'surface.{subject}.shift': 0, 'surface.{subject}.specularity': 0}\n",
      "waiting for camera.altitude 0.1 -> 0\n",
      "[Errno 2] No such file or directory: 'convert': 'convert'\n",
      "Stopping server\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d_views_screenshots/PA_meridians/sub-01_lateral_pivot_inflated.png',\n",
       " '3d_views_screenshots/PA_meridians/sub-01_left_inflated.png',\n",
       " '3d_views_screenshots/PA_meridians/sub-01_right_inflated.png',\n",
       " '3d_views_screenshots/PA_meridians/sub-01_bottom_inflated.png',\n",
       " '3d_views_screenshots/PA_meridians/sub-01_back_inflated.png',\n",
       " '3d_views_screenshots/PA_meridians/sub-01_flatmap_flatmap.png',\n",
       " '3d_views_screenshots/PA_meridians/sub-01_flatmap_flatmap.png']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_path = '3d_views_screenshots/PA_meridians'\n",
    "if not os.path.exists(fig_path):  # check if path exists\n",
    "        os.makedirs(fig_path)\n",
    "\n",
    "cortex.export.save_3d_views(images['angle_half_hemi'], \n",
    "             base_name=os.path.join(fig_path,'sub-%s'%(sj)), \n",
    "             list_angles=['lateral_pivot','left','right','bottom','back',\n",
    "                         'left','right'],\n",
    "                  list_surfaces=['inflated','inflated','inflated','inflated','inflated',\n",
    "                                'flatmap','flatmap'],\n",
    "                  viewer_params=dict(labels_visible=[],\n",
    "                                     overlays_visible=['rois','sulci']),\n",
    "                  size=(1024 * 4, 768 * 4), trim=True, sleep=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO ADD MOTOR AND HAVE BOTH IN BROWSER ###\n",
    "# FIRST DRAW PICTOGRAMS IN OVERLAY ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
